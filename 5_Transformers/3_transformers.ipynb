{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "num_steps = 1000\n",
    "lr = 1e-3\n",
    "context_length = 32\n",
    "embedding_size = 64 # change this in future and test if it works \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "vocab_size = 39\n",
    "num_heads = 4\n",
    "n_blocks = 4  # num_layers or 'N' in transformer block\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8' ) as f:\n",
    "    data = f.read().lower()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<UNK>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(data)))\n",
    "chars.remove('3')\n",
    "chars.append('<UNK>')\n",
    "print(chars)\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '$': 3,\n",
       " '&': 4,\n",
       " \"'\": 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " ':': 9,\n",
       " ';': 10,\n",
       " '?': 11,\n",
       " 'a': 12,\n",
       " 'b': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " 'e': 16,\n",
       " 'f': 17,\n",
       " 'g': 18,\n",
       " 'h': 19,\n",
       " 'i': 20,\n",
       " 'j': 21,\n",
       " 'k': 22,\n",
       " 'l': 23,\n",
       " 'm': 24,\n",
       " 'n': 25,\n",
       " 'o': 26,\n",
       " 'p': 27,\n",
       " 'q': 28,\n",
       " 'r': 29,\n",
       " 's': 30,\n",
       " 't': 31,\n",
       " 'u': 32,\n",
       " 'v': 33,\n",
       " 'w': 34,\n",
       " 'x': 35,\n",
       " 'y': 36,\n",
       " 'z': 37,\n",
       " '<UNK>': 38}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for t in range(len(chars)):\n",
    "    vocab[t] = chars[t]\n",
    "\n",
    "rev_vocab = {v:k for k,v in vocab.items()}\n",
    "rev_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 16, 23, 23, 26, 38]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    out = []\n",
    "    for t in text:\n",
    "        out.append(rev_vocab.get(t, 38)) # 38 is the <UNK>\n",
    "    return out\n",
    "\n",
    "\n",
    "encode('hello1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w-tr!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode(nums):\n",
    "    out = ''\n",
    "    for i in nums:\n",
    "        out += vocab.get(i, '<UNK>')\n",
    "    return out\n",
    "\n",
    "\n",
    "decode([34,  7, 31, 29, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1003854, 111540)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = torch.tensor(encode(data), dtype=torch.long)\n",
    "train_data = encoded_data[:int(0.9*len(encoded_data))]\n",
    "test_data = encoded_data[int(0.9*len(encoded_data)):]\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_batch(split='train'):\n",
    "    data = train_data if split == 'train' else test_data\n",
    "    start = torch.randint(0,len(data)-context_length-1,(batch_size,))\n",
    "    outx = torch.stack([data[i:i+context_length] for i in start]).to(device)\n",
    "    if split == 'test': return outx\n",
    "    outy = torch.stack([data[i+1:i+context_length+1] for i in start]).to(device)\n",
    "    if split == 'train': return outx, outy\n",
    "\n",
    "get_batch('train')[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.K = nn.Linear(embedding_size, head_size, bias=False)\n",
    "        self.V = nn.Linear(embedding_size, head_size, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # BatchSize Time ContextSize\n",
    "        k = self.K(x)\n",
    "        q = self.Q(x)\n",
    "        v = self.V(x)\n",
    "\n",
    "        out = q@k.transpose(-2,-1)\n",
    "        out = out*embedding_size**-0.5\n",
    "        \n",
    "        out = out*torch.tril(torch.ones(32,32)) # probably can be improved. No need for element wise mul here\n",
    "        out = out.masked_fill(out==0, float('-inf'))# masking\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        # maybe add dropout here\n",
    "        out = out@v\n",
    "        return out\n",
    "\n",
    "# h = Head(100)\n",
    "# h(torch.rand(16, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads) ])\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(embedding_size,embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout1(self.l1(out))\n",
    "        return out\n",
    "\n",
    "# mul_head = MultiHeadAttention(12, 32)\n",
    "# mh = mul_head(torch.rand(16, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            nn.Linear(embedding_size, 4 * embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 *embedding_size, embedding_size),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embedding_size ,num_heads):\n",
    "        super().__init__()\n",
    "        head_size = embedding_size//num_heads\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads, head_size)\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "        self.ff = FeedForwardLayer(embedding_size)\n",
    "    \n",
    "    def foward(self, x):\n",
    "        out = self.multi_head_attention(self.ln1(x)) + out\n",
    "        out = self.ff(self.ln1(x)) + out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.positional_embedding_table = nn.Embedding(context_length, embedding_size)\n",
    "        self.transformer_blocks = nn.Sequential(*[TransformerBlock(embedding_size, num_heads) for i in range(n_blocks)])\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_embedding = self.embedding_table(idx)\n",
    "        pos_embedding = self.positional_embedding_table(torch.arange(T, device=device))\n",
    "\n",
    "        tot_embedding = tok_embedding + pos_embedding\n",
    "\n",
    "        out = self.transformer_blocks(tot_embedding)\n",
    "        out = self.ln(out)\n",
    "        logits = self.lm_head(out)\n",
    "\n",
    "        if targets:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        \n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 0.206375 M\n"
     ]
    }
   ],
   "source": [
    "model = Transformers().to(device)\n",
    "print('Parameters:',sum(p.numel() for p in model.parameters())/1e6, 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss():pass\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
